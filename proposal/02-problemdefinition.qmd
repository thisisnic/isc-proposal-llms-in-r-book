---
bibliography: references.bib
---

# The Problem

LLMs and AI are having a huge impact on software engineering and data science, and the tools available, model capabilities, and advice and guidance on their use are all rapidly changing.  Much of the avaialble guidance is scattered across difference locations, such as model vendor websites, R package vignettes, and blog posts by community members.  

While existing resources provide valuable solution-oriented information, such as how to use specific functions and packages, R users are left without a clear framework for understanding the problem space. This involves decisions about whether and when to use LLMs; for example, in using them to explain results to stakeholders, embedding code in analysis pipelines while maintaining data privacy, and generating R code.

As a result, users must piece together knowledge from multiple sources, with no obvious place to start.  There is real risk of misapplying methods, and wasting time and effort. The problem exists for many R users including data scientists, analysts, and educators who want to integrate LLMs into analysis, teaching, and tooling. 

This problem could be solved by consolidating best practices, explanations, and guidance into a single structured resource, with further links to other resources. This would give R users confidence in having a reliable foundation for experimenting with LLMs, and strengthen R's position as a community invested in open, inclusive and responsible use of technology.

There is some existing work in this space, such as [Luis D. Verde Arregoitia's guide to tooling for LLMs in R](https://luisdva.github.io/llmsr-book/), but this focuses on available tools rather than concepts or context.

Due to the speed at which things are moving in LLMs/AI, it's unlikely that this kind of resource would be a good candidate for a permanent, static reference like a book published via a traditional publisher.  Instead, value lies in creating a living, community-oriented resource that can be updated as packages evolve, new methods emerge, and best practices shift. By structuring it as an open, ongoing work rather than a fixed text, the book can remain relevant to R users and avoid the obsolescence that traditional publications in this domain are likely to face.

This approach aligns with successful R Consortium-funded social infrastructure projects, including translation and internationalisation efforts that have strengthened community accessibility, and educational initiatives that have built lasting knowledge resources for the R ecosystem. Like these precedents, this project prioritises sustainable, community-owned infrastructure over static deliverables.